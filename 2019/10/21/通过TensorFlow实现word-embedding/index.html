<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
<script data-ad-client="ca-pub-1831623022964098" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/C_Meng.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/C_Meng.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/C_Meng.png">
  <link rel="mask-icon" href="/images/C_Meng.png" color="#222">
  <meta name="google-site-verification" content="-WnFIB8dOJwdL5b4iIfg6bNp1o-p5XnbyMIPVWXr6N0">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"imonce.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="word2vec的方法主要分为CBOW（Continuous Bag Of Words）和skip-gram（n-gram）两大类。 两种方法互为镜像。简单来说，CBOW是通过上下文预测中间值来进行训练的，skip-gram是通过中间值预测上下文来进行训练的。 这里，我们使用skip-gram的方法。 python脚本IDE: jupyter notebook 123456789101112131">
<meta name="keywords" content="tensorflow,word2vec,n-gram,skip-gram,Embedding">
<meta property="og:type" content="article">
<meta property="og:title" content="通过TensorFlow实现word embedding">
<meta property="og:url" content="https://imonce.github.io/2019/10/21/通过TensorFlow实现word-embedding/index.html">
<meta property="og:site_name" content="C_Meng PSNA">
<meta property="og:description" content="word2vec的方法主要分为CBOW（Continuous Bag Of Words）和skip-gram（n-gram）两大类。 两种方法互为镜像。简单来说，CBOW是通过上下文预测中间值来进行训练的，skip-gram是通过中间值预测上下文来进行训练的。 这里，我们使用skip-gram的方法。 python脚本IDE: jupyter notebook 123456789101112131">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://raw.githubusercontent.com/imonce/imgs/master/20191021100559.png">
<meta property="og:updated_time" content="2020-01-04T08:02:49.210Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="通过TensorFlow实现word embedding">
<meta name="twitter:description" content="word2vec的方法主要分为CBOW（Continuous Bag Of Words）和skip-gram（n-gram）两大类。 两种方法互为镜像。简单来说，CBOW是通过上下文预测中间值来进行训练的，skip-gram是通过中间值预测上下文来进行训练的。 这里，我们使用skip-gram的方法。 python脚本IDE: jupyter notebook 123456789101112131">
<meta name="twitter:image" content="https://raw.githubusercontent.com/imonce/imgs/master/20191021100559.png">

<link rel="canonical" href="https://imonce.github.io/2019/10/21/通过TensorFlow实现word-embedding/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'default'
  };
</script>

  <title>通过TensorFlow实现word embedding | C_Meng PSNA</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">C_Meng PSNA</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Never wait for the storm to pass, just dance in the rain.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">214</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">32</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">137</span></a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/imonce" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="default">
    <link itemprop="mainEntityOfPage" href="https://imonce.github.io/2019/10/21/通过TensorFlow实现word-embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/C_Meng.png">
      <meta itemprop="name" content="C_Meng">
      <meta itemprop="description" content="This site is primarily used for personal note-taking, and some content are from open access, with sources clearly indicated. If there are any copyright issues, please feel free to contact us. 本站主要用于个人笔记记录，存在部分内容引用，均已表明出处，如存在版权问题，敬请联系。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="C_Meng PSNA">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          通过TensorFlow实现word embedding
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-21 10:16:01" itemprop="dateCreated datePublished" datetime="2019-10-21T10:16:01+08:00">2019-10-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-04 16:02:49" itemprop="dateModified" datetime="2020-01-04T16:02:49+08:00">2020-01-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/10/21/通过TensorFlow实现word-embedding/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/10/21/通过TensorFlow实现word-embedding/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>word2vec的方法主要分为CBOW（Continuous Bag Of Words）和skip-gram（n-gram）两大类。</p>
<p>两种方法互为镜像。简单来说，CBOW是通过上下文预测中间值来进行训练的，skip-gram是通过中间值预测上下文来进行训练的。</p>
<p>这里，我们使用skip-gram的方法。</p>
<h1 id="python脚本"><a href="#python脚本" class="headerlink" title="python脚本"></a>python脚本</h1><p>IDE: jupyter notebook</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicPatternEmbedding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.url = <span class="string">'http://mattmahoney.net/dc/'</span></span><br><span class="line">        self.data_index = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        self.vocabulary_size = <span class="number">5000</span></span><br><span class="line">        </span><br><span class="line">        self.batch_size = <span class="number">128</span></span><br><span class="line">        self.embedding_size = <span class="number">128</span>  <span class="comment"># Dimension of the embedding vector.</span></span><br><span class="line">        self.skip_window = <span class="number">1</span>       <span class="comment"># How many words to consider left and right.</span></span><br><span class="line">        self.num_skips = <span class="number">2</span>         <span class="comment"># How many times to reuse an input to generate a label.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># We pick a random validation set to sample nearest neighbors. Here we limit the</span></span><br><span class="line">        <span class="comment"># validation samples to the words that have a low numeric ID, which by</span></span><br><span class="line">        <span class="comment"># construction are also the most frequent.</span></span><br><span class="line">        self.valid_size = <span class="number">16</span>     <span class="comment"># Random set of words to evaluate similarity on.</span></span><br><span class="line">        self.valid_window = <span class="number">100</span>  <span class="comment"># Only pick dev samples in the head of the distribution.</span></span><br><span class="line">        <span class="comment"># choose 16 numbers from 0 to 99 randomly</span></span><br><span class="line">        self.valid_examples = np.random.choice(self.valid_window, self.valid_size, replace=<span class="keyword">False</span>)</span><br><span class="line">        self.num_sampled = <span class="number">64</span>    <span class="comment"># Number of negative examples to sample.</span></span><br><span class="line">        self.num_steps = <span class="number">10001</span></span><br><span class="line">        </span><br><span class="line">        self.final_embedding = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        self.graph = tf.Graph()</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># download and verify the dataset file</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maybe_download</span><span class="params">(self, filename, expected_bytes)</span>:</span></span><br><span class="line">        <span class="comment"># If the dataset file is not under the current path, download it directly</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filename):</span><br><span class="line">            filename, _ = urllib.request.urlretrieve(self.url + filename, filename)</span><br><span class="line">        <span class="comment"># get dataset file infomationn</span></span><br><span class="line">        statinfo = os.stat(filename)</span><br><span class="line">        <span class="comment"># verify file size</span></span><br><span class="line">        <span class="keyword">if</span> statinfo.st_size == expected_bytes:</span><br><span class="line">            print(<span class="string">'Found and verified'</span>, filename)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(statinfo.st_size)</span><br><span class="line">            <span class="keyword">raise</span> Exception(</span><br><span class="line">                <span class="string">'Failed to verify '</span> + filename + <span class="string">'. Can you get to it with a browser?'</span>)</span><br><span class="line">        <span class="keyword">return</span> filename</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># read the data from zip into a list of strings</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> zipfile.ZipFile(filename) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># separate by default separators, that is, all null characters, including spaces, newlines (\n), tabs (\t), etc.</span></span><br><span class="line">            data = tf.compat.as_str(f.read(f.namelist()[<span class="number">0</span>])).split()</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># process raw inputs into a dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="comment"># add unknown words into count list</span></span><br><span class="line">        count = [[<span class="string">'UNK'</span>, <span class="number">-1</span>]]</span><br><span class="line">        <span class="comment"># count the words list and add the pairs (word_name, number) into count list</span></span><br><span class="line">        count.extend(collections.Counter(words).most_common(self.vocabulary_size - <span class="number">1</span>))</span><br><span class="line">        dictionary = dict()</span><br><span class="line">        <span class="comment"># create a dictionary of the words with serial number</span></span><br><span class="line">        <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</span><br><span class="line">            dictionary[word] = len(dictionary)</span><br><span class="line">        data = list()</span><br><span class="line">        unk_count = <span class="number">0</span></span><br><span class="line">        <span class="comment"># convert the word list into a number list, 0 for unknown words</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">                index = dictionary[word]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                index = <span class="number">0</span></span><br><span class="line">                unk_count += <span class="number">1</span></span><br><span class="line">            data.append(index)</span><br><span class="line">        <span class="comment"># update the number of UNK</span></span><br><span class="line">        count[<span class="number">0</span>][<span class="number">1</span>] = unk_count</span><br><span class="line">        <span class="comment"># generate a new dictionary by exchanging key and value</span></span><br><span class="line">        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</span><br><span class="line">        <span class="keyword">return</span> data, count, dictionary, reversed_dictionary</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># function to generate a training batch for the skip-gram model</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="comment"># make sure the data length is OK</span></span><br><span class="line">        <span class="keyword">assert</span> self.batch_size % self.num_skips == <span class="number">0</span></span><br><span class="line">        <span class="keyword">assert</span> self.num_skips &lt;= <span class="number">2</span> * self.skip_window</span><br><span class="line"></span><br><span class="line">        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)</span><br><span class="line">        labels = np.ndarray(shape=(self.batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">        span = <span class="number">2</span> * self.skip_window + <span class="number">1</span>  <span class="comment"># [ skip_window target skip_window ]</span></span><br><span class="line">        <span class="comment"># create a new double-ended queue to store the buffer</span></span><br><span class="line">        buffer = collections.deque(maxlen=span)</span><br><span class="line">        <span class="comment"># data_index indicates the end point of the current window</span></span><br><span class="line">        <span class="keyword">if</span> self.data_index + span &gt; len(data):</span><br><span class="line">            data_index = <span class="number">0</span></span><br><span class="line">        buffer.extend(data[self.data_index:self.data_index + span])</span><br><span class="line">        self.data_index += span</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.batch_size // self.num_skips):</span><br><span class="line">            target = self.skip_window  <span class="comment"># target label at the center of the buffer</span></span><br><span class="line">            targets_to_avoid = [self.skip_window]</span><br><span class="line">            <span class="comment"># sample num_skips batches and labels, optimizable</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.num_skips):</span><br><span class="line">                <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">                    target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># avoid sampling to the same target</span></span><br><span class="line">                targets_to_avoid.append(target)</span><br><span class="line">                <span class="comment"># each batch item stands for input</span></span><br><span class="line">                batch[i * self.num_skips + j] = buffer[self.skip_window]</span><br><span class="line">                <span class="comment"># each label item stands for ground truth</span></span><br><span class="line">                labels[i * self.num_skips + j, <span class="number">0</span>] = buffer[target]</span><br><span class="line">            <span class="keyword">if</span> self.data_index == len(data):</span><br><span class="line">                buffer[:] = data[:span]</span><br><span class="line">                self.data_index = span</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                buffer.append(data[self.data_index])</span><br><span class="line">                self.data_index += <span class="number">1</span></span><br><span class="line">        <span class="comment"># Backtrack a little bit to avoid skipping words in the end of a batch</span></span><br><span class="line">        self.data_index = self.data_index - span</span><br><span class="line">        <span class="keyword">return</span> batch, labels</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, data, reverse_dictionary)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> self.graph.as_default():</span><br><span class="line">            train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])</span><br><span class="line">            train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, <span class="number">1</span>])</span><br><span class="line">            valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ops and variables pinned to the CPU</span></span><br><span class="line">            <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">                <span class="comment"># Look up embeddings for inputs.</span></span><br><span class="line">                embeddings = tf.Variable(tf.random_uniform([self.vocabulary_size, self.embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">                <span class="comment"># according to embeddings, the 128-dimensional vector corresponding to the input word(train inputs) was extracted</span></span><br><span class="line">                embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Construct the variables for the NCE loss</span></span><br><span class="line">                nce_weights = tf.Variable(tf.truncated_normal([self.vocabulary_size, self.embedding_size], stddev=<span class="number">1.0</span> / math.sqrt(self.embedding_size)))</span><br><span class="line">                nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))</span><br><span class="line">            <span class="comment"># Compute the average NCE loss for the batch.</span></span><br><span class="line">            <span class="comment"># tf.nce_loss automatically draws a new sample of the negative labels each</span></span><br><span class="line">            <span class="comment"># time we evaluate the loss.</span></span><br><span class="line">            loss = tf.reduce_mean(</span><br><span class="line">                tf.nn.nce_loss(weights=nce_weights,</span><br><span class="line">                             biases=nce_biases,</span><br><span class="line">                             labels=train_labels,</span><br><span class="line">                             inputs=embed,</span><br><span class="line">                             num_sampled=self.num_sampled,</span><br><span class="line">                             num_classes=self.vocabulary_size))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Construct the SGD optimizer using a learning rate of 1.0.</span></span><br><span class="line">            optimizer = tf.train.GradientDescentOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute the cosine similarity between minibatch examples and all embeddings.</span></span><br><span class="line">            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="keyword">True</span>))</span><br><span class="line">            normalized_embeddings = embeddings / norm</span><br><span class="line">            valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)</span><br><span class="line">            similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Add variable initializer.</span></span><br><span class="line">            init = tf.global_variables_initializer()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">with</span> tf.Session(graph = self.graph) <span class="keyword">as</span> session:</span><br><span class="line">            init.run()</span><br><span class="line">            average_loss = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> xrange(self.num_steps):</span><br><span class="line">                batch_inputs, batch_labels = self.generate_batch(data)</span><br><span class="line">                feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment"># we perform one update step by evaluating the optimizer op (including it</span></span><br><span class="line">                <span class="comment"># in the list of returned values for session.run()</span></span><br><span class="line">                _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">                average_loss += loss_val</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                        average_loss /= <span class="number">2000</span></span><br><span class="line">                    <span class="comment"># the average loss is an estimate of the loss over the last 2000 batches.</span></span><br><span class="line">                    print(<span class="string">'Average loss at step '</span>, step, <span class="string">': '</span>, average_loss)</span><br><span class="line">                    average_loss = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># output the most similar eight words to the screen</span></span><br><span class="line">                <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">                    sim = similarity.eval()</span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(self.valid_size):</span><br><span class="line">                        valid_word = reverse_dictionary[self.valid_examples[i]]</span><br><span class="line">                        top_k = <span class="number">8</span>  <span class="comment"># number of nearest neighbors</span></span><br><span class="line">                        nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k + <span class="number">1</span>]</span><br><span class="line">                        log_str = <span class="string">'Nearest to %s:'</span> % valid_word</span><br><span class="line">                        <span class="keyword">for</span> k <span class="keyword">in</span> xrange(top_k):</span><br><span class="line">                            close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">                            log_str = <span class="string">'%s %s,'</span> % (log_str, close_word)</span><br><span class="line">                        print(log_str)</span><br><span class="line">                        </span><br><span class="line">            self.final_embeddings = normalized_embeddings.eval()</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># visualize the embeddings</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(self, low_dim_embs, labels, filename=<span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">'More labels than embeddings'</span></span><br><span class="line">        plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))  <span class="comment"># in inches</span></span><br><span class="line">        <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">            x, y = low_dim_embs[i, :]</span><br><span class="line">            plt.scatter(x, y)</span><br><span class="line">            plt.annotate(label,</span><br><span class="line">                            xy=(x, y),</span><br><span class="line">                            xytext=(<span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">                            textcoords=<span class="string">'offset points'</span>,</span><br><span class="line">                            ha=<span class="string">'right'</span>,</span><br><span class="line">                            va=<span class="string">'bottom'</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="comment">#plt.savefig(filename)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        bpe = BasicPatternEmbedding()</span><br><span class="line">        filename = bpe.maybe_download(<span class="string">'text8.zip'</span>,<span class="number">31344016</span>)</span><br><span class="line">        vocabulary = bpe.read_data(filename)</span><br><span class="line">        print(<span class="string">'Data size'</span>, len(vocabulary))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'vocabulary:'</span>, vocabulary[:<span class="number">10</span>])</span><br><span class="line">        </span><br><span class="line">        data, count, dictionary, reverse_dictionary = bpe.build_dataset(vocabulary)</span><br><span class="line">        <span class="keyword">del</span> vocabulary  <span class="comment"># Hint to reduce memory.</span></span><br><span class="line">        print(<span class="string">'Most common words (+UNK)'</span>, count[:<span class="number">5</span>])</span><br><span class="line">        print(<span class="string">'Sample data'</span>, data[:<span class="number">10</span>], [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> data[:<span class="number">10</span>]])</span><br><span class="line">        </span><br><span class="line">        batch, labels = bpe.generate_batch(data)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">            print(batch[i], reverse_dictionary[batch[i]], <span class="string">'-&gt;'</span>, labels[i, <span class="number">0</span>], reverse_dictionary[labels[i, <span class="number">0</span>]])</span><br><span class="line">        <span class="keyword">print</span> (dictionary[<span class="string">'a'</span>], dictionary[<span class="string">'as'</span>], dictionary[<span class="string">'term'</span>])</span><br><span class="line">        </span><br><span class="line">        bpe.train(data, reverse_dictionary)</span><br><span class="line">        </span><br><span class="line">        tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>, method=<span class="string">'exact'</span>)</span><br><span class="line">        plot_only = <span class="number">300</span></span><br><span class="line">        low_dim_embs = tsne.fit_transform(bpe.final_embeddings[:plot_only, :])</span><br><span class="line"></span><br><span class="line">        labels = [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> xrange(plot_only)]</span><br><span class="line">        bpe.plot_with_labels(low_dim_embs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> ImportError:</span><br><span class="line">        print(<span class="string">'Please install sklearn, matplotlib, and scipy to show embeddings.'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Found and verified text8.zip
Data size 17005207
vocabulary: [&#39;anarchism&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;, &#39;against&#39;]
Most common words (+UNK) [[&#39;UNK&#39;, 2735459], (&#39;the&#39;, 1061396), (&#39;of&#39;, 593677), (&#39;and&#39;, 416629), (&#39;one&#39;, 411764)]
Sample data [0, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] [&#39;UNK&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;, &#39;against&#39;]
3081 originated -&gt; 12 as
3081 originated -&gt; 0 UNK
12 as -&gt; 6 a
12 as -&gt; 3081 originated
6 a -&gt; 195 term
6 a -&gt; 12 as
195 term -&gt; 2 of
195 term -&gt; 6 a
6 12 195
Average loss at step  0 :  185.77481079101562
Nearest to it: confidence, doesn, theatre, came, gulf, cultural, sites, corps,
Nearest to use: buried, grave, observation, dust, batman, security, hungarian, opens,
Nearest to at: warrior, total, rivers, yards, reaction, extinction, exclusively, eu,
Nearest to if: emergency, present, developing, dates, life, for, pennsylvania, genesis,
Nearest to between: grant, execution, generally, power, official, interpreted, hiv, binary,
Nearest to people: unlikely, mainly, prussian, dedicated, shot, spending, dangerous, pick,
Nearest to states: forward, racing, begins, printed, follow, vacuum, study, mythology,
Nearest to by: rulers, protestant, marvel, republic, zero, letters, researchers, amiga,
Nearest to american: hit, stores, managed, practiced, intermediate, retrieved, moreover, unique,
Nearest to world: leadership, decay, culture, false, vii, et, dialogue, gave,
Nearest to but: denominations, passing, according, germans, medical, emperors, working, grant,
Nearest to an: removed, marxist, experts, ac, eugene, bones, tree, ne,
Nearest to were: coat, facing, grammar, storage, teach, covering, solomon, circuit,
Nearest to to: plant, supporting, pay, pp, shell, problem, acids, post,
Nearest to be: judah, photo, films, both, senate, woman, villages, eating,
Nearest to used: legislative, hero, private, organ, spaces, vice, top, trivia,
Average loss at step  2000 :  22.257665908694268
Average loss at step  4000 :  5.249317247629166
Average loss at step  6000 :  4.652066127896309
Average loss at step  8000 :  4.529780765414238
Average loss at step  10000 :  4.432040006399155
Nearest to it: he, came, votes, matters, doesn, whole, confidence, continues,
Nearest to use: alien, buried, security, hungarian, grave, dust, batman, amount,
Nearest to at: in, killed, appearance, extinction, mathbf, rivers, eu, pronunciation,
Nearest to if: life, molecules, emergency, dates, present, pennsylvania, for, rates,
Nearest to between: eight, execution, vs, of, hiv, grant, official, documentary,
Nearest to people: UNK, mainly, dedicated, selection, unlikely, shot, fact, dangerous,
Nearest to states: forward, racing, cover, arithmetic, study, vacuum, vs, begins,
Nearest to by: and, as, in, infant, co, manufacturer, with, campaign,
Nearest to american: hit, importance, austin, entry, depending, retrieved, vs, intermediate,
Nearest to world: culture, leadership, UNK, false, mathbf, skills, et, titled,
Nearest to but: and, medical, working, was, connecticut, vs, europeans, denominations,
Nearest to an: the, ac, plant, challenge, experts, necessary, lake, marxist,
Nearest to were: jpg, are, facing, covering, manual, circuit, opposite, test,
Nearest to to: ends, and, plant, in, office, into, supporting, agave,
Nearest to be: iso, shorter, judah, self, painter, also, dependent, assistance,
Nearest to used: opposition, hero, private, illinois, legislative, regime, breaking, repeated,</code></pre>
<p><img src="https://raw.githubusercontent.com/imonce/imgs/master/20191021100559.png"></p>
<h1 id="相关函数说明"><a href="#相关函数说明" class="headerlink" title="相关函数说明"></a>相关函数说明</h1><h2 id="Tensor-eval"><a href="#Tensor-eval" class="headerlink" title="Tensor.eval()"></a>Tensor.eval()</h2><p>.eval() 其实就是tf.Tensor的Session.run() 的另外一种写法，但两者有差别</p>
<ol>
<li>eval(): 将字符串string对象转化为有效的表达式参与求值运算返回计算结果</li>
<li>eval()也是启动计算的一种方式。基于Tensorflow的基本原理，首先需要定义图，然后计算图，其中计算图的函数常见的有run()函数，如sess.run()。同样eval()也是此类函数，</li>
<li>要注意的是，eval()只能用于tf.Tensor类对象，也就是有输出的Operation，写作Tensor.eval()。对于没有输出的Operation, 可以用.run()或者Session.run()；Session.run()没有这个限制。</li>
</ol>
<h2 id="np-argsort"><a href="#np-argsort" class="headerlink" title="np.argsort()"></a>np.argsort()</h2><p>argsort函数返回的是数组值从小到大的索引值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.array([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argsort(x)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h2 id="tf-reduce-sum"><a href="#tf-reduce-sum" class="headerlink" title="tf.reduce_sum()"></a>tf.reduce_sum()</h2><p>reduce_sum( ) 是求和函数，在 tensorflow 里面，计算的都是 tensor，可以通过调整 axis =0,1 的维度来控制求和维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = tf.constant([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x)</span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x, <span class="number">0</span>)</span><br><span class="line">[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x, <span class="number">1</span>)</span><br><span class="line">[<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x, <span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">[[<span class="number">3</span>],[<span class="number">3</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.reduce_sum(x, [<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>

<h2 id="tf-nn-nce-loss"><a href="#tf-nn-nce-loss" class="headerlink" title="tf.nn.nce_loss()"></a>tf.nn.nce_loss()</h2><p>假设nce_loss之前的输入数据是K维的，一共有N个类，那么</p>
<p>weight.shape = (N, K)</p>
<p>bias.shape = (N)</p>
<p>inputs.shape = (batch_size, K)</p>
<p>labels.shape = (batch_size, num_true)</p>
<p>num_true : 实际的正样本个数</p>
<p>num_sampled: 采样出多少个负样本</p>
<p>num_classes = N</p>
<p>sampled_values: 采样出的负样本，如果是None，就会用不同的sampler去采样。待会儿说sampler是什么。</p>
<p>remove_accidental_hits: 如果采样时不小心采样到的负样本刚好是正样本，要不要干掉</p>
<p>partition_strategy：对weights进行embedding_lookup时并行查表时的策略。TF的embeding_lookup是在CPU里实现的，这里需要考虑多线程查表时的锁的问题</p>
<p>nce_loss的实现逻辑如下：</p>
<p>_compute_sampled_logits: 通过这个函数计算出正样本和采样出的负样本对应的output和label</p>
<p>sigmoid_cross_entropy_with_logits: 通过 sigmoid cross entropy来计算output和label的loss，从而进行反向传播。这个函数把最后的问题转化为了num_sampled+num_real个两类分类问题，然后每个分类问题用了交叉熵的损伤函数，也就是logistic regression常用的损失函数。TF里还提供了一个softmax_cross_entropy_with_logits的函数，和这个有所区别。</p>
<p>在训练过程中，作为input的embed也会被自动更新</p>
<h2 id="tf-nn-embedding-lookup"><a href="#tf-nn-embedding-lookup" class="headerlink" title="tf.nn.embedding_lookup()"></a>tf.nn.embedding_lookup()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Signature:</span></span><br><span class="line">tf.nn.embedding_lookup(params, ids, partition_strategy=<span class="string">'mod'</span>, name=<span class="keyword">None</span>, validate_indices=<span class="keyword">True</span>, max_norm=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># Docstring:</span></span><br><span class="line"><span class="comment"># Looks up `ids` in a list of embedding tensors.</span></span><br></pre></td></tr></table></figure>

<p>是根据 ids 中的id，寻找 params 中的第id行。比如 ids=[1,3,5]，则找出params中第1，3，5行，组成一个tensor返回。</p>
<p>embedding_lookup不是简单的查表，params 对应的向量是可以训练的，训练参数个数应该是 feature_num * embedding_size，即前文表述的embedding层权重矩阵，就是说 lookup 的是一种全连接层。</p>
<p>partition_strategy 为张量编号方式，在张量存在多维时起作用，编号的方式有两种，”mod”（默认） 和 “div”。</p>
<p>假设：一共有三个tensor [a,b,c] 作为params 参数，所有tensor的第 0 维上一共有 10 个项目（id 0 ~ 9）。</p>
<p>“mod” : (id) mod len(params) 得到 多少就把 id 分到第几个tensor里面</p>
<ul>
<li>a 依次分到id： 0 3 6 9</li>
<li>b 依次分到id： 1 4 7</li>
<li>c 依次分到id： 2 5 8</li>
</ul>
<p>“div” : (id) div len(params) 可以理解为依次排序，但是这两种切分方式在无法均匀切分的情况下都是将前(max_id+1)%len(params)个 partition 多分配一个元素.</p>
<ul>
<li>a 依次分到id： 0 1 2 3</li>
<li>b 依次分到id： 4 5 6</li>
<li>c 依次分到id： 7 8 9</li>
</ul>
<h2 id="tf-SparseTensor"><a href="#tf-SparseTensor" class="headerlink" title="tf.SparseTensor()"></a>tf.SparseTensor()</h2><p>构造稀疏向量矩阵，每一行为一个样本</p>
<p>SparseTensor(indices, values, dense_shape)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SparseTensor(indices=[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">2</span>]], values=[<span class="number">1</span>, <span class="number">2</span>], dense_shape=[<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># represents the dense tensor</span></span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>

<blockquote>
<p>reference:<br><a href="https://blog.csdn.net/qoopqpqp/article/details/76037334" target="_blank" rel="noopener">https://blog.csdn.net/qoopqpqp/article/details/76037334</a><br><a href="https://segmentfault.com/a/1190000015287066?utm_source=tag-newest" target="_blank" rel="noopener">https://segmentfault.com/a/1190000015287066?utm_source=tag-newest</a><br><a href="https://blog.csdn.net/u012193416/article/details/83349138" target="_blank" rel="noopener">https://blog.csdn.net/u012193416/article/details/83349138</a><br><a href="https://blog.csdn.net/qq_36092251/article/details/79684721" target="_blank" rel="noopener">https://blog.csdn.net/qq_36092251/article/details/79684721</a><br><a href="https://gshtime.github.io/2018/06/01/tensorflow-embedding-lookup-sparse/" target="_blank" rel="noopener">https://gshtime.github.io/2018/06/01/tensorflow-embedding-lookup-sparse/</a></p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
              <a href="/tags/word2vec/" rel="tag"># word2vec</a>
              <a href="/tags/n-gram/" rel="tag"># n-gram</a>
              <a href="/tags/skip-gram/" rel="tag"># skip-gram</a>
              <a href="/tags/Embedding/" rel="tag"># Embedding</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/10/10/一文读懂Curry-Howard同构/" rel="prev" title="一文读懂Curry-Howard同构">
      <i class="fa fa-chevron-left"></i> 一文读懂Curry-Howard同构
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/10/21/3小时精通lxml-etree-Python中xml的读取、解析、生成和查找/" rel="next" title="3小时精通lxml.etree:Python中xml的读取、解析、生成和查找">
      3小时精通lxml.etree:Python中xml的读取、解析、生成和查找 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1831623022964098"
     data-ad-slot="5648729926"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#python脚本"><span class="nav-number">1.</span> <span class="nav-text">python脚本</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#相关函数说明"><span class="nav-number">2.</span> <span class="nav-text">相关函数说明</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor-eval"><span class="nav-number">2.1.</span> <span class="nav-text">Tensor.eval()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#np-argsort"><span class="nav-number">2.2.</span> <span class="nav-text">np.argsort()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-reduce-sum"><span class="nav-number">2.3.</span> <span class="nav-text">tf.reduce_sum()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-nn-nce-loss"><span class="nav-number">2.4.</span> <span class="nav-text">tf.nn.nce_loss()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-nn-embedding-lookup"><span class="nav-number">2.5.</span> <span class="nav-text">tf.nn.embedding_lookup()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-SparseTensor"><span class="nav-number">2.6.</span> <span class="nav-text">tf.SparseTensor()</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="C_Meng"
      src="/images/C_Meng.png">
  <p class="site-author-name" itemprop="name">C_Meng</p>
  <div class="site-description" itemprop="description">This site is primarily used for personal note-taking, and some content are from open access, with sources clearly indicated. If there are any copyright issues, please feel free to contact us. 本站主要用于个人笔记记录，存在部分内容引用，均已表明出处，如存在版权问题，敬请联系。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">137</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">214</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/imonce" title="GitHub → https://github.com/imonce" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:imonce@outlook.com" title="E-Mail → mailto:imonce@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/3912390829/profile?topnav=1&wvr=6&is_all=1" title="Weibo → https://weibo.com/3912390829/profile?topnav=1&wvr=6&is_all=1" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/moescee" title="Twitter → https://twitter.com/moescee" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1831623022964098"
     data-ad-slot="5648729926"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">C_Meng</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>


  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1279293837&web_id=1279293837"></script>
  </div>






      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  <script src="/js/local-search.js"></script>








<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'hBUFmUyAMBRRHULc0Y4SPPzw-gzGzoHsz',
      appKey     : 'WBQPPWjmwGWqvQcLqprPq0xs',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
